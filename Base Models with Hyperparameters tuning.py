# -*- coding: utf-8 -*-
"""Github code 22222.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12C683f5k8B0_8fvhkSOCPxo4iVgukiTy

#For Severe Depression
"""

# import csv
# import nltk
# from nltk.corpus import wordnet
# from nltk.tokenize import word_tokenize
# nltk.download('punkt')
# nltk.download('wordnet')

# # Function for synonym replacement
# def synonym_replacement(sentence, n):
#     tokens = word_tokenize(sentence)
#     augmented_sentences = []
#     for _ in range(n):
#         new_tokens = []
#         for token in tokens:
#             synsets = wordnet.synsets(token)
#             if synsets:
#                 synonym = synsets[0].lemmas()[0].name()
#                 new_tokens.append(synonym)
#             else:
#                 new_tokens.append(token)
#         augmented_sentence = ' '.join(new_tokens)
#         augmented_sentences.append(augmented_sentence)
#     return augmented_sentences

# # Read the CSV file
# file_path = '/content/train_data.csv'  # Replace with the actual file path
# rows = []
# with open(file_path, 'r') as file:
#     reader = csv.reader(file)
#     for row in reader:
#         rows.append(row)

# # Augment the text data with label "severe" and insert augmented lines
# augmented_rows = []
# for row in rows:
#     if len(row) == 3 and row[2] == 'severe':  # Check if it's a valid row with three columns and label "severe"
#         pid, text_data, label = row
#         augmented_sentences = synonym_replacement(text_data, n=4)  # Augment with 3 sentences
#         for augmented_sentence in augmented_sentences:
#             augmented_row = [pid, augmented_sentence, label]
#             augmented_rows.append(augmented_row)

# # Append the augmented rows to the existing CSV file
# with open(file_path, 'a', newline='') as file:
#     writer = csv.writer(file)
#     writer.writerows(augmented_rows)

# print("Data augmentation completed and augmented lines inserted into the CSV file.")

"""#Augmentation for "not Depression"
"""

# import csv
# import nltk
# from nltk.corpus import wordnet
# from nltk.tokenize import word_tokenize

# # Function for synonym replacement
# def synonym_replacement(sentence, n):
#     tokens = word_tokenize(sentence)
#     augmented_sentences = []
#     for _ in range(n):
#         new_tokens = []
#         for token in tokens:
#             synsets = wordnet.synsets(token)
#             if synsets:
#                 synonym = synsets[0].lemmas()[0].name()
#                 new_tokens.append(synonym)
#             else:
#                 new_tokens.append(token)
#         augmented_sentence = ' '.join(new_tokens)
#         augmented_sentences.append(augmented_sentence)
#     return augmented_sentences

# # Read the CSV file
# file_path = '/content/train_data (1).csv'  # Replace with the actual file path
# rows = []
# with open(file_path, 'r') as file:
#     reader = csv.reader(file)
#     for row in reader:
#         rows.append(row)

# # Augment the text data for the first 300 rows with label "not depression" and insert augmented lines
# augmented_rows = []
# augmented_count = 0
# for row in rows:
#     if augmented_count >= 300:
#         break

#     if len(row) == 3 and row[2] == 'not depression':  # Check if it's a valid row with three columns and label "not depression"
#         pid, text_data, label = row
#         augmented_sentences = synonym_replacement(text_data, n=3)  # Augment with 3 sentences
#         for augmented_sentence in augmented_sentences:
#             augmented_row = [pid, augmented_sentence, label]
#             augmented_rows.append(augmented_row)
#             augmented_count += 1

# # Append the augmented rows to the existing CSV file
# with open(file_path, 'a', newline='') as file:
#     writer = csv.writer(file)
#     writer.writerows(augmented_rows)

# print("Data augmentation completed for the first 300 rows with the label 'not depression', and augmented lines inserted into the CSV file.")



"""#Sampling"""

# import csv
# import nltk
# import pandas as pd
# from nltk.corpus import wordnet
# from nltk.tokenize import word_tokenize
# from imblearn.over_sampling import SMOTE
# from sklearn.feature_extraction.text import CountVectorizer
# nltk.download('punkt')
# nltk.download('wordnet')

# # Function for synonym replacement
# def synonym_replacement(sentence, n):
#     tokens = word_tokenize(sentence)
#     augmented_sentences = []
#     for _ in range(n):
#         new_tokens = []
#         for token in tokens:
#             synsets = wordnet.synsets(token)
#             if synsets:
#                 synonym = synsets[0].lemmas()[0].name()
#                 new_tokens.append(synonym)
#             else:
#                 new_tokens.append(token)
#         augmented_sentence = ' '.join(new_tokens)
#         augmented_sentences.append(augmented_sentence)
#     return augmented_sentences

# # Read the CSV file
# file_path = '/content/train_data.csv'  # Replace with the actual file path
# data = pd.read_csv(file_path)

# # Preprocess the text data
# corpus = data['Text data'].tolist()
# vectorizer = CountVectorizer()
# X = vectorizer.fit_transform(corpus)
# y = data['Label']

# # Augment the data with SMOTE
# smote = SMOTE(sampling_strategy='auto')
# X_oversampled, y_oversampled = smote.fit_resample(X, y)

# # Convert the oversampled data back to DataFrame
# augmented_data = pd.DataFrame(X_oversampled.todense())
# augmented_data['Label'] = y_oversampled

# # Append the augmented data to the existing CSV file
# augmented_data.to_csv(file_path, mode='a', header=False, index=False)

# print("Data augmentation using SMOTE completed, and augmented lines inserted into the CSV file.")

"""#Imbalanced-learn"""

# !pip install -U scikit-learn

# import csv
# import nltk
# import pandas as pd
# from nltk.corpus import wordnet
# from nltk.tokenize import word_tokenize
# from imblearn.over_sampling import RandomOverSampler
# from sklearn.feature_extraction.text import CountVectorizer
# nltk.download('punkt')
# nltk.download('wordnet')

# # Function for synonym replacement
# def synonym_replacement(sentence, n):
#     tokens = word_tokenize(sentence)
#     augmented_sentences = []
#     for _ in range(n):
#         new_tokens = []
#         for token in tokens:
#             synsets = wordnet.synsets(token)
#             if synsets:
#                 synonym = synsets[0].lemmas()[0].name()
#                 new_tokens.append(synonym)
#             else:
#                 new_tokens.append(token)
#         augmented_sentence = ' '.join(new_tokens)
#         augmented_sentences.append(augmented_sentence)
#     return augmented_sentences

# # Read the CSV file
# file_path = '/content/train_data.csv'  # Replace with the actual file path
# data = pd.read_csv(file_path)

# # Preprocess the text data
# corpus = data['Text data'].tolist()
# vectorizer = CountVectorizer()
# X = vectorizer.fit_transform(corpus)
# y = data['Label']

# # Augment the data with RandomOverSampler
# ros = RandomOverSampler(sampling_strategy='auto')
# X_oversampled, y_oversampled = ros.fit_resample(X, y)

# # Convert the oversampled data back to DataFrame
# feature_names = vectorizer.get_feature_names_out()
# augmented_data = pd.DataFrame(X_oversampled.todense(), columns=feature_names)
# augmented_data['Label'] = y_oversampled

# # Append the augmented data to the existing CSV file
# augmented_data.to_csv(file_path, mode='a', header=False, index=False)

# print("Data augmentation using RandomOverSampler completed, and augmented lines inserted into the CSV file.")

"""###########|
#https://www.section.io/engineering-education/using-imbalanced-learn-to-handle-imbalanced-text-data/
"""

import pandas as pd

df = pd.read_csv("/content/train_data.csv", sep=",", header=None)

print(df.shape)

df.head()

df.rename(columns={2: 'Label', 1: 'Text data'}, inplace=True)

df = df.drop(0).reset_index(drop=True)
df.head()

df.isnull().sum()

df['Label'].value_counts()

df['length'] = df['Text data'].apply(lambda x: len(x))

df.head()

!pip install nltk

import nltk
import string

nltk.download('punkt')
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize
import re

def convert_to_lower(text):
    return text.lower()

def remove_numbers(text):
    number_pattern = r'\d+'
    without_number = re.sub(pattern=number_pattern, repl=" ", string=text)
    return without_number


def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))


def remove_stopwords(text):
    removed = []
    stop_words = list(stopwords.words("english"))
    tokens = word_tokenize(text)
    for i in range(len(tokens)):
        if tokens[i] not in stop_words:
            removed.append(tokens[i])
    return " ".join(removed)

def remove_extra_white_spaces(text):
    single_char_pattern = r'\s+[a-zA-Z]\s+'
    without_sc = re.sub(pattern=single_char_pattern, repl=" ", string=text)
    return without_sc

def lemmatizing(text):
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text)
    for i in range(len(tokens)):
        lemma_word = lemmatizer.lemmatize(tokens[i])
        tokens[i] = lemma_word
    return " ".join(tokens)

import nltk
nltk.download('wordnet')

df['Text data'] = df['Text data'].apply(lambda x: convert_to_lower(x))
df['Text data'] = df['Text data'].apply(lambda x: remove_numbers(x))
df['Text data'] = df['Text data'].apply(lambda x: remove_punctuation(x))
df['Text data'] = df['Text data'].apply(lambda x: remove_punctuation(x))
df['Text data'] = df['Text data'].apply(lambda x: remove_extra_white_spaces(x))
df['Text data'] = df['Text data'].apply(lambda x: lemmatizing(x))

df['length_after_cleaning'] = df['Text data'].apply(lambda x: len(x))

df.head()

label_map = {
    'not depression': 0,
    'moderate': 1,
    'severe': 2
}

df['Label'] = df['Label'].replace(label_map)

df['Label'] = df['Label'].replace({'not depression': 0, 'moderate': 1, 'severe': 2})

df.head()

df['Label'].value_counts()

from sklearn.feature_extraction.text import TfidfVectorizer
tf_wb= TfidfVectorizer()

X_tf = tf_wb.fit_transform(df['Text data'])

import numpy as np

X_tf = X_tf.toarray()

print(X_tf)

from sklearn.naive_bayes import GaussianNB

from sklearn.model_selection import train_test_split
X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, df['Label'].values, test_size=0.3)

NB = GaussianNB()

NB.fit(X_train_tf, y_train_tf)

NB_pred= NB.predict(X_test_tf)
print(NB_pred)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test_tf, NB_pred))

!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE
SM = SMOTE(sampling_strategy={
    0: 6123,
    1: 6123,
    2: 6123
})
X_train_smote, y_train_smote = SM.fit_resample(X_train_tf, y_train_tf)

# feature_names = tf_wb.get_feature_names_out()
# sentences = [' '.join([feature_names[idx] for idx, val in enumerate(sentence) if val != 0])
#              for sentence in X_train_smote]

!pip install imbalanced-learn
!pip install gensim==3.8.3

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from collections import Counter
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# Assuming you have a DataFrame named 'df' with 'Text data' and 'Label' columns

X_train, X_test, y_train, y_test = train_test_split(df['Text data'], df['Label'].values, test_size=0.30)
Counter(y_train)

# Train Word2Vec model
sentences = [simple_preprocess(sentence) for sentence in X_train]
model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)

# Convert sentences to Word2Vec embeddings
def sentence_to_word2vec(sentence):
    return [model.wv[word] for word in sentence if word in model.wv.key_to_index]

X_train_word2vec = [sentence_to_word2vec(simple_preprocess(sentence)) for sentence in X_train]
X_test_word2vec = [sentence_to_word2vec(simple_preprocess(sentence)) for sentence in X_test]

# Convert Word2Vec embeddings to numpy arrays
import numpy as np

X_train_word2vec = np.array([np.mean(sentence, axis=0) if sentence else np.zeros(model.vector_size) for sentence in X_train_word2vec])
X_test_word2vec = np.array([np.mean(sentence, axis=0) if sentence else np.zeros(model.vector_size) for sentence in X_test_word2vec])

# Apply SMOTE oversampling
SM = SMOTE(sampling_strategy={
    0: 3167,
    1: 3167,
    2: 3167
})
X_train_smote, y_train_smote = SM.fit_resample(X_train_word2vec, y_train)

Counter(y_train_smote)

# from sklearn.naive_bayes import GaussianNB
# from sklearn.metrics import accuracy_score

# # Train the Gaussian Naive Bayes classifier
# nb = GaussianNB()
# nb.fit(X_train_word2vec, y_train)

# # Predict on the test set
# y_preds = nb.predict(X_test_word2vec)

# # Evaluate the accuracy
# print(y_preds)
# print(accuracy_score(y_test, y_preds))

# X_train_smote

!pip install transformers
!pip install torch
!pip install --upgrade transformers

"""#XLMROberta"""

!pip install sentencepiece

# Install required packages
!pip install transformers
!pip install torch
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup

# Assuming you have a DataFrame named 'df' with 'Text data' and 'Label' columns

# Preprocessing steps (same as before)
# ...

# Train-Test Split (same as before)
# ...

# Tokenization using XLM-RoBERTa tokenizer
model_name = "xlm-roberta-base"
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)

# Tokenize input data
X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors="pt", max_length=300)
X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors="pt", max_length=300)

# Convert labels to tensors
y_train_tensor = torch.tensor(y_train)
y_test_tensor = torch.tensor(y_test)

# Load the XLM-RoBERTa model for sequence classification
model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Define hyperparameters
batch_size = 16
learning_rate = 2e-5  # Adjust learning rate for XLM-RoBERTa
num_epochs = 10
validation_steps = 100

# Prepare DataLoader for training and testing
train_dataset = TensorDataset(X_train_tokens.input_ids, X_train_tokens.attention_mask, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(X_test_tokens.input_ids, X_test_tokens.attention_mask, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Setup optimizer and learning rate schedule
optimizer = AdamW(model.parameters(), lr=learning_rate)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training and evaluation
for epoch in range(num_epochs):
    model.train()
    for step, batch in enumerate(train_dataloader):
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        if (step + 1) % validation_steps == 0:
            model.eval()
            with torch.no_grad():
                total_correct = 0
                total_samples = 0
                for test_batch in test_dataloader:
                    test_input_ids, test_attention_mask, test_labels = test_batch
                    test_input_ids, test_attention_mask, test_labels = test_input_ids.to(device), test_attention_mask.to(device), test_labels.to(device)

                    test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
                    test_logits = test_outputs.logits
                    test_predictions = torch.argmax(test_logits, dim=1)

                    total_correct += (test_predictions == test_labels).sum().item()
                    total_samples += test_labels.size(0)

                accuracy = total_correct / total_samples
                print(f"Epoch {epoch + 1}/{num_epochs} - Step {step + 1}/{len(train_dataloader)} - Test Accuracy: {accuracy:.4f}")

            model.train()

# Final evaluation
model.eval()
all_predictions = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Final Test Accuracy: {accuracy:.4f}")
XLMROBERTa_preds=all_predictions
# Calculate classification report
report = classification_report(all_labels, all_predictions, target_names=["Class 0", "Class 1", "Class 2"])
print("Classification Report:\n", report)

"""#Deberta"""

# Install required packages
!pip install transformers
!pip install torch
from sklearn.metrics import classification_report
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import DebertaTokenizer, DebertaForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup

# Assuming you have a DataFrame named 'df' with 'Text data' and 'Label' columns

# Preprocessing steps (same as before)
# ...

# Train-Test Split (same as before)
# ...

# Tokenization using DeRoBERTa tokenizer
model_name = "microsoft/deberta-base"
tokenizer = DebertaTokenizer.from_pretrained(model_name)

# Tokenize input data
X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors="pt", max_length=300)
X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors="pt", max_length=300)

# Convert labels to tensors
y_train_tensor = torch.tensor(y_train)
y_test_tensor = torch.tensor(y_test)

# Load the DeRoBERTa model for sequence classification
model = DebertaForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Define hyperparameters
batch_size = 16
learning_rate = 5e-6
dropout = 0.1
weight_decay = 0.1
num_epochs = 10
validation_steps = 100

# Prepare DataLoader for training and testing
train_dataset = TensorDataset(X_train_tokens.input_ids, X_train_tokens.attention_mask, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(X_test_tokens.input_ids, X_test_tokens.attention_mask, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Setup optimizer and learning rate schedule
optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training and evaluation
for epoch in range(num_epochs):
    model.train()
    for step, batch in enumerate(train_dataloader):
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        if (step + 1) % validation_steps == 0:
            model.eval()
            with torch.no_grad():
                total_correct = 0
                total_samples = 0
                for test_batch in test_dataloader:
                    test_input_ids, test_attention_mask, test_labels = test_batch
                    test_input_ids, test_attention_mask, test_labels = test_input_ids.to(device), test_attention_mask.to(device), test_labels.to(device)

                    test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
                    test_logits = test_outputs.logits
                    test_predictions = torch.argmax(test_logits, dim=1)

                    total_correct += (test_predictions == test_labels).sum().item()
                    total_samples += test_labels.size(0)

                accuracy = total_correct / total_samples
                print(f"Epoch {epoch + 1}/{num_epochs} - Step {step + 1}/{len(train_dataloader)} - Test Accuracy: {accuracy:.4f}")

            model.train()

# Final evaluation
model.eval()
all_predictions = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Final Test Accuracy: {accuracy:.4f}")
DeRoberta_preds=all_predictions
# Calculate classification report
report = classification_report(all_labels, all_predictions, target_names=["Class 0", "Class 1", "Class 2"])
print("Classification Report:\n", report)

"""#Bert"""

# Install required packages
!pip install transformers
!pip install torch
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup

# Assuming you have a DataFrame named 'df' with 'Text data' and 'Label' columns

# Preprocessing steps (same as before)
# ...

# Train-Test Split (same as before)
# ...

# Tokenization using DistilBERT tokenizer
model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

# Tokenize input data
X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors="pt", max_length=300)
X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors="pt", max_length=300)

# Convert labels to tensors
y_train_tensor = torch.tensor(y_train)
y_test_tensor = torch.tensor(y_test)

# Load the DistilBERT model for sequence classification
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Define hyperparameters
batch_size = 16
learning_rate = 5e-5  # Slightly larger learning rate for DistilBERT
num_epochs = 10
validation_steps = 100

# Prepare DataLoader for training and testing
train_dataset = TensorDataset(X_train_tokens.input_ids, X_train_tokens.attention_mask, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(X_test_tokens.input_ids, X_test_tokens.attention_mask, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Setup optimizer and learning rate schedule
optimizer = AdamW(model.parameters(), lr=learning_rate)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training and evaluation
for epoch in range(num_epochs):
    model.train()
    for step, batch in enumerate(train_dataloader):
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        if (step + 1) % validation_steps == 0:
            model.eval()
            with torch.no_grad():
                total_correct = 0
                total_samples = 0
                for test_batch in test_dataloader:
                    test_input_ids, test_attention_mask, test_labels = test_batch
                    test_input_ids, test_attention_mask, test_labels = test_input_ids.to(device), test_attention_mask.to(device), test_labels.to(device)

                    test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
                    test_logits = test_outputs.logits
                    test_predictions = torch.argmax(test_logits, dim=1)

                    total_correct += (test_predictions == test_labels).sum().item()
                    total_samples += test_labels.size(0)

                accuracy = total_correct / total_samples
                print(f"Epoch {epoch + 1}/{num_epochs} - Step {step + 1}/{len(train_dataloader)} - Test Accuracy: {accuracy:.4f}")

            model.train()

# Final evaluation
model.eval()
all_predictions = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Final Test Accuracy: {accuracy:.4f}")
BERT_preds=all_predictions
# Calculate classification report
report = classification_report(all_labels, all_predictions, target_names=["Class 0", "Class 1", "Class 2"])
print("Classification Report:\n", report)

"""#Roberta"""

# Install required packages
!pip install transformers
!pip install torch
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup

# Assuming you have a DataFrame named 'df' with 'Text data' and 'Label' columns

# Preprocessing steps (same as before)
# ...

# Train-Test Split (same as before)
# ...

# Tokenization using RoBERTa tokenizer
model_name = "roberta-base"
tokenizer = RobertaTokenizer.from_pretrained(model_name)

# Tokenize input data
X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors="pt", max_length=300)
X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors="pt", max_length=300)

# Convert labels to tensors
y_train_tensor = torch.tensor(y_train)
y_test_tensor = torch.tensor(y_test)

# Load the RoBERTa model for sequence classification
model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Define hyperparameters
batch_size = 16
learning_rate = 1e-5  # Adjust learning rate for RoBERTa
num_epochs = 10
validation_steps = 100

# Prepare DataLoader for training and testing
train_dataset = TensorDataset(X_train_tokens.input_ids, X_train_tokens.attention_mask, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(X_test_tokens.input_ids, X_test_tokens.attention_mask, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Setup optimizer and learning rate schedule
optimizer = AdamW(model.parameters(), lr=learning_rate)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training and evaluation
for epoch in range(num_epochs):
    model.train()
    for step, batch in enumerate(train_dataloader):
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        if (step + 1) % validation_steps == 0:
            model.eval()
            with torch.no_grad():
                total_correct = 0
                total_samples = 0
                for test_batch in test_dataloader:
                    test_input_ids, test_attention_mask, test_labels = test_batch
                    test_input_ids, test_attention_mask, test_labels = test_input_ids.to(device), test_attention_mask.to(device), test_labels.to(device)

                    test_outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
                    test_logits = test_outputs.logits
                    test_predictions = torch.argmax(test_logits, dim=1)

                    total_correct += (test_predictions == test_labels).sum().item()
                    total_samples += test_labels.size(0)

                accuracy = total_correct / total_samples
                print(f"Epoch {epoch + 1}/{num_epochs} - Step {step + 1}/{len(train_dataloader)} - Test Accuracy: {accuracy:.4f}")

            model.train()

# Final evaluation
model.eval()
all_predictions = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Final Test Accuracy: {accuracy:.4f}")
Roberta_preds=all_predictions
# Calculate classification report
report = classification_report(all_labels, all_predictions, target_names=["Class 0", "Class 1", "Class 2"])
print("Classification Report:\n", report)



"""#Ensamble method"""

from sklearn.metrics import classification_report

# Assuming you have a list of predictions from base models
base_model_predictions = [BERT_preds, XLMROBERTa_preds, Roberta_preds, DeRoberta_preds]  # Replace with your own predictions

# Voting: Select the most common prediction for each example
ensemble_predictions = []
for i in range(len(base_model_predictions[0])):
    single_example_predictions = [model_preds[i] for model_preds in base_model_predictions]
    most_common_prediction = max(single_example_predictions, key=single_example_predictions.count)
    ensemble_predictions.append(most_common_prediction)

print(classification_report(all_labels, ensemble_predictions))

from collections import Counter

# Assuming you have a list of predictions from base models
base_model_predictions = [XLMROBERTa_preds, Roberta_preds, DeRoberta_preds]  # Replace with your own predictions

# Averaging: Compute the average prediction for each example
ensemble_predictions = []
for i in range(len(base_model_predictions[0])):
    single_example_predictions = [model_preds[i] for model_preds in base_model_predictions]
    label_counts = Counter(single_example_predictions)
    most_common_label = label_counts.most_common(1)[0][0]
    ensemble_predictions.append(most_common_label)

print(classification_report(all_labels, ensemble_predictions))

from collections import Counter

# Assuming you have a list of predictions from base models
base_model_predictions = [BERT_preds, Roberta_preds, DeRoberta_preds]  # Replace with your own predictions

# Calculate weights based on F1-scores (higher F1-score gets a higher weight)
f1_scores = [f1_score(all_labels, preds, average='weighted') for preds in base_model_predictions]
total_f1 = sum(f1_scores)
weights = [f1 / total_f1 for f1 in f1_scores]

# Weighted Majority Voting: Compute the ensemble prediction for each example
ensemble_predictions = []
for i in range(len(base_model_predictions[0])):
    single_example_predictions = [model_preds[i] for model_preds in base_model_predictions]
    weighted_majority = max(single_example_predictions, key=lambda x: sum(weights[j] for j, pred in enumerate(single_example_predictions) if pred == x))
    ensemble_predictions.append(weighted_majority)

print(classification_report(all_labels, ensemble_predictions))

