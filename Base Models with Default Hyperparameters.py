# -*- coding: utf-8 -*-
"""Github code 111111 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBksLTh1kfohO6zPcWRv8AHfxMjMwD77
"""

!pip install transformers
!pip install --upgrade accelerate

!wget -O data.csv "https://figshare.com/ndownloader/files/4988956"
!pip install emoji

import nltk
nltk.download('punkt')

"""## Task 1. Data Cleaning, Preprocessing, and splitting [15 points]
The `data` environment contains the SMILE dataset loaded into a pandas dataframe object. Our dataset has three columns: id, tweet, and label. The `tweet` column contains the raw scraped tweet and the `label` column contains the annotated emotion category. Each tweet is labelled with one of the following emotion labels:
- 'nocode', 'not-relevant'
- 'happy', 'happy|surprise', 'happy|sad'
- 'angry', 'disgust|angry', 'disgust'
- 'sad', 'sad|disgust', 'sad|disgust|angry'
- 'surprise'

### Task 1a. Label Consolidation [ 3 points]
As we can see above the annotated categories are complex. Several tweets express complex emotions like (e.g. 'happy|sad') or multiple emotions (e.g. 'sad|disgust|angry'). The first things we need to do is clean up our dataset by removing complex examples and consolidating others so that we have a clean set of emotions to predict.

For Task 1a., write code which does the following:
1. Drops all rows which have the label "happy|sad", "happy|surprise", 'sad|disgust|angry', and 'sad|angry'.
2. Re-label 'nocode' and 'not-relevant' as 'no-emotion'.
3. Re-label 'disgust|angry' and 'disgust' as 'angry'.
4. Re-label 'sad|disgust' as 'sad'.

Your updated `data' dataframe should have 3,062 rows and 5 label categories (no-emotion, happy, angry, sad, and surprise).

"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# Refrence- https://note.nkmk.me/en/python-pandas-dataframe-rename/#:~:text=You%20can%20use%20the%20rename,change%20column%2Findex%20name%20individually.&text=Specify%20the%20original%20name%20and,is%20for%20the%20index%20name.
#Refrence- https://www.statology.org/pandas-drop-rows-with-value/
#Refrence- https://www.w3schools.com/python/pandas/ref_df_replace.asp#:~:text=The%20replace()%20method%20replaces,case%20of%20the%20specified%20value.

#Reading CSV
train_data= pd.read_csv("/content/drive/MyDrive/AI_Project /train_data1.csv")

val_data= pd.read_csv("/content/drive/MyDrive/AI_Project /dev_data.csv")

test_data= pd.read_csv("/content/drive/MyDrive/AI_Project /test_data.csv")

# #Duplicating the row
# data=pd.DataFrame([['611857364396965889', '@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap', 'nocode']], columns=data.columns).append(data)

# #Adding the Column Labels
# data=data.rename(columns={'611857364396965889': 'Id', '@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap': 'tweet', 'nocode':'label'})

train_data

"""### Task 1b. Tweet Cleaning and Processing [10 points]
Raw tweets are noisy. Consider the example below:
```
'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up #foundationcourses. ðŸ˜ '
```
The mention @tateliverpool and hashtag #BobandRoberta are extra noise that don't directly help with understanding the emotion of the text. The accompanying emoji can be useful but needs to be decoded to it text form :angry: first.

For this task you will fill complete the `preprocess_tweet` function below with the following preprocessing steps:
1. Lower case all text
2. De-emoji the text
3. Remove all hashtags, mentions, and urls
4. Remove all non-alphabet characters except the followng punctuations: period, exclamation mark, and question mark

Hints:
- For step 2 (de-emoji), consider using the python [emoji](https://carpedm20.github.io/emoji/docs/) library. The `emoji.demojize` method will convert all emojis to plain text. The `emoji` library is installed in cell [52].
- Follow the processing steps in order. For example calling nltk's word_tokenize before removing hashtags and mentions will end up creating seperate tokens for @ and # and cause problems.

To get full credit for this task, the Test 1b must pass. Only modify the  cell containing the `preprocess_tweet` function and do not alter the testing code block.

After you are satisfied with your code, run the tests. code to ensure your function works as expected. This cell will also create a new column called `cleaned_tweet` and apply the `preproces_tweet` function to all the examples in the dataset.
"""

#Refrence- https://catriscode.com/2021/03/02/extracting-or-removing-mentions-and-hashtags-in-tweets-using-python/
#Refrence- https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python

import emoji
import re

def preprocess_tweet(tweet: str) -> str:
  """
  Function takes a raw tweet and performs the following processing steps:
  1. Lower case all text
  2. De-emoji the text
  3. Remove all hashtags, mentions, and urls
  4. Remove all non-alphabet characters except the followng punctuations: period, exclamation mark, and question mark
  """
  # 1. Lower case all text
  tweet=tweet.lower()

  # 2. De-emoji the text
  tweet=emoji.demojize(tweet, delimiters=("",""))

  # 3. Remove all hashtags, mentions, and urls
  tweet=re.sub("@[A-Za-z0-9_]+","", tweet) # Removing Mentions
  tweet=re.sub("#[A-Za-z0-9_]+","", tweet) # Removing hashtag
  tweet=re.sub(r'http\S+', '', tweet) # Removing urls

  # 4. Remove all non-alphabet characters except the followng punctuations: period, exclamation mark, and question mark
  tweet= re.sub(r'[^\w\s\.\?\!]','',tweet)
  tweet=tweet.replace('_','')

  #5. Removing Awkward spaces
  tweet=tweet.split()
  tweet=" ".join(tweet)

  return tweet


test_tweet = "'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up! #foundationcourses ðŸ˜ '"
print(preprocess_tweet(test_tweet))

"""### Task 1b Test
Run the cell below to evaluate your code. To get full credit for this task, your code must pass all tests. Any alteration of the testing code will automatically result in 0 points.
"""

from xgboost.training import train
# Do NOT modify the code below.
# Create new column with cleaned tweets. We will use this for the subsequent tasks
train_data["cleaned_data"] = train_data["Text data"].apply(preprocess_tweet)
val_data["cleaned_data"] = val_data["text data"].apply(preprocess_tweet)
test_data["cleaned_data"] = test_data["Text data"].apply(preprocess_tweet)

labels=test_data['Label']
test = test_data.drop('Label', axis=1)
test1=test_data['cleaned_data']
train= train_data['cleaned_data']
train_lab=train_data['Label']
val= val_data.drop('Class labels', axis=1)

test_data.columns

from accelerate.state import PartialState
from accelerate.state import PartialState

"""#XLM ROBERTa"""

!pip install sentencepiece

import torch
from torch.utils.data import Dataset
from transformers import XLMRobertaTokenizer
from transformers import XLMRobertaForSequenceClassification
from transformers import Trainer
from transformers import TrainingArguments
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score

class SentimentDataset(Dataset):
    def __init__(self, encodings: dict, labels: torch.Tensor):
        self.encodings = encodings
        self.labels = labels

    def __len__(self) -> int:
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx: int) -> dict:
        e = {k: v[idx] for k, v in self.encodings.items()}
        e["label"] = self.labels[idx]
        return e

AT4 = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

le = LabelEncoder()

# Transform the train labels to numerical representations
train_labels = le.fit_transform(train_data["Label"])
val_labels = le.fit_transform(val_data["Class labels"])
test_labels = le.fit_transform(test_data["Label"])

# Train Inputs
train_encodings = AT4(
    train_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
train_dataset = SentimentDataset(train_encodings, torch.tensor(train_labels))

# Val Inputs
val_encodings = AT4(
    val_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
val_dataset = SentimentDataset(val_encodings, torch.tensor(val_labels))

# Test Inputs
test_encodings = AT4(
    test_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
test_dataset = SentimentDataset(test_encodings, torch.tensor(test_labels))

num_labels = 3  # Set the number of labels for classification
model_name = "xlm-roberta-base"  # Choose the XLM-Roberta model variant
model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type='cosine',
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    fp16=True,
)

XLMROBERTa_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=lambda data: {"input_ids": torch.stack([item["input_ids"] for item in data]),
                               "attention_mask": torch.stack([item["attention_mask"] for item in data]),
                               "labels": torch.tensor([item["label"] for item in data])},
    compute_metrics=lambda pred: {"accuracy": (pred.predictions.argmax(-1) == pred.label_ids).mean()}
)

# Train the model
XLMROBERTa_trainer.train()

# Get predictions on test dataset
predictions = XLMROBERTa_trainer.predict(test_dataset)
test_predictions = predictions.predictions.argmax(-1)

# Convert torch tensor to numpy array for XGBoost
train_X = XLMROBERTa_trainer.get_train_dataloader().dataset.encodings["input_ids"].numpy()
train_y = XLMROBERTa_trainer.get_train_dataloader().dataset.labels.numpy()
val_X = val_dataset.encodings["input_ids"].numpy()
val_y = val_dataset.labels.numpy()

# Train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(train_X, train_y)

# Make predictions using XGBoost model
xgb_test_predictions = xgb_model.predict(test_encodings["input_ids"].numpy())

# Calculate accuracy for XGBoost model
xgb_test_accuracy = accuracy_score(test_labels, xgb_test_predictions)
xgb_test_accuracy

#Refrence- Advanced NLP Lab 03 Solutions provided on Blackboard
import numpy as np
from sklearn.metrics import classification_report

XLMROBERTa_preds = XLMROBERTa_trainer.predict(test_dataset)
# print(preds)

XLMROBERTa_preds = le.inverse_transform(np.argmax(XLMROBERTa_preds.predictions, axis=1))
# print(preds)
# print(val_data["Class labels"])
print(classification_report(test_data["Label"].tolist(), XLMROBERTa_preds))

"""## Task 2. Transfer Learning with DistilBERT

For this task you will finetune a pretrained language model (DistilBERT) using the huggingface `transformers` library. For this task you will need to:
- Encode the tweets using the BERT tokenizer
- Create pytorch datasets for for the train, val and test datasets
- Finetune the distilbert model for 5 epochs
- Extract predictions from the model's output logits and convert them into the emotion labels.
- Generate a classification report on the predictions.

Ensure you are running the notebook in Google Colab with the gpu runtime enabled for this section.
"""

#Refrence- Advanced NLP Lab 01 Solutions provided on Blackboard

import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import Trainer
from transformers import TrainingArguments
from sklearn.preprocessing import LabelEncoder

# Your Code here

class SentimentDataset(Dataset):

    def __init__(self, encodings: dict):
        self.encodings = encodings

    def __len__(self) -> int:
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx: int) -> dict:
        e = {k: v[idx] for k,v in self.encodings.items()}
        return e


AT= AutoTokenizer.from_pretrained("distilbert-base-uncased")


le = LabelEncoder()

# Fit the LabelEncoder on the train labels
# tr= le.fit(train["label"])
# vl= le.fit(val["label"])
# ts= le.fit(test["label"])

# Transform the train labels to numerical representations
train_labels =  le.fit_transform(train_data["Label"])

val_labels = le.fit_transform(val_data["Class labels"])

test_labels = le.fit_transform(test_data["Label"])



# Train Inputs
BERT_train_encodings = AT(
    train_data["cleaned_data"].tolist(),
    padding=True,           # pad all inputs to max length
    max_length=128,         # Bert max is 512, we choose 128 due to compute limitations
    return_tensors="pt",    # Return format pytorch tensor
    truncation=True
)
BERT_train_encodings["label"] = torch.tensor(train_labels)  # Update train inputs with labels
train_dataset = SentimentDataset(BERT_train_encodings)

# Val Inputs
val_encodings = AT(
    val_data["cleaned_data"].tolist(),
    padding=True,           # pad all inputs to max length
    max_length=128,         # Bert max is 512, we choose 128 due to compute limitations
    return_tensors="pt",     # Return format pytorch tensor
    truncation=True
)
val_encodings["label"] = torch.tensor(val_labels)  # Update train inputs with labels
val_dataset = SentimentDataset(val_encodings)


# Test Inputs
test_encodings = AT(
    test_data["cleaned_data"].tolist(),
    padding=True,           # pad all inputs to max length
    max_length=128,         # Bert max is 512, we choose 128 due to compute limitations
    return_tensors="pt",     # Return format pytorch tensor
    truncation=True
)
test_y = test_labels
test_dataset = SentimentDataset(test_encodings)

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type='cosine',
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    fp16=True,
)

BERT_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

BERT_trainer.train()

#Refrence- Advanced NLP Lab 03 Solutions provided on Blackboard
import numpy as np
from sklearn.metrics import classification_report

BERT_preds = BERT_trainer.predict(test_dataset)
# print(preds)

BERT_preds = le.inverse_transform(np.argmax(BERT_preds.predictions, axis=1))
# print(preds)
# print(val_data["Class labels"])
print(classification_report(test_data["Label"].tolist(), BERT_preds))

"""#Roberta"""

!pip install transformers
!pip install xgboost

import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import Trainer
from transformers import TrainingArguments
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score

class SentimentDataset(Dataset):
    def __init__(self, encodings: dict):
        self.encodings = encodings

    def __len__(self) -> int:
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx: int) -> dict:
        e = {k: v[idx] for k, v in self.encodings.items()}
        return e

AT1 = AutoTokenizer.from_pretrained("roberta-base")

le = LabelEncoder()

# Transform the train labels to numerical representations
train_labels = le.fit_transform(train_data["Label"])
val_labels = le.fit_transform(val_data["Class labels"])
test_labels = le.fit_transform(test_data["Label"])

# Train Inputs
Roberta_train_encodings = AT1(
    train_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
Roberta_train_encodings["label"] = torch.tensor(train_labels)
train_dataset = SentimentDataset(Roberta_train_encodings)

# Val Inputs
Roberta_val_encodings = AT1(
    val_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
Roberta_val_encodings["label"] = torch.tensor(val_labels)
val_dataset = SentimentDataset(Roberta_val_encodings)

# Test Inputs
test_encodings = AT1(
    test_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
test_y = test_labels
test_dataset = SentimentDataset(test_encodings)

num_labels = 3  # Set the number of labels for classification
model_name = "roberta-base"  # Choose the RoBERTa model variant
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type='cosine',
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    fp16=True,
)

Roberta_trainer1 = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train the model
Roberta_trainer1.train()

# Get predictions on test dataset
predictions = Roberta_trainer1.predict(test_dataset)
test_predictions = predictions.predictions.argmax(-1)

# Convert torch tensor to numpy array for XGBoost
train_X = Roberta_trainer1.get_train_dataloader().dataset.encodings["input_ids"].numpy()
train_y = Roberta_trainer1.get_train_dataloader().dataset.encodings["label"].numpy()
val_X = val_dataset.encodings["input_ids"].numpy()
val_y = val_dataset.encodings["label"].numpy()

# Train XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(train_X, train_y)

# Make predictions using XGBoost model
xgb_test_predictions = xgb_model.predict(test_encodings["input_ids"].numpy())

# Calculate accuracy for XGBoost model
xgb_test_accuracy = accuracy_score(test_labels, xgb_test_predictions)
xgb_test_accuracy

#Refrence- Advanced NLP Lab 03 Solutions provided on Blackboard
import numpy as np
from sklearn.metrics import classification_report

Roberta_preds = Roberta_trainer1.predict(test_dataset)
# print(preds)

Roberta_preds = le.inverse_transform(np.argmax(Roberta_preds.predictions, axis=1))
# print(preds)
# print(val_data["Class labels"])
print(classification_report(test_data["Label"].tolist(), Roberta_preds))

"""#SVM"""

# from sklearn import svm
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report

# # Step 1: Preprocess your text data and prepare your labels
# # Assuming you have your text data stored in a list called 'texts' and corresponding labels in 'labels'

# # Step 2: Convert text data into numerical feature vectors
# vectorizer = TfidfVectorizer()  # You can customize the vectorizer based on your specific requirements
# X_train = vectorizer.fit_transform(train).toarray()
# X_test = vectorizer.transform(test1).toarray()
# y_train = train_lab
# y_test = labels

# # Step 3: Split the data into training and testing sets
# # X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

# # Step 4: Train the SVM model
# svm_model = svm.SVC(kernel='linear')
# svm_model.fit(X_train, y_train)

# # Step 5: Evaluate the model
# SVM_y_pred = svm_model.predict(X_test)
# print(classification_report(y_test, SVM_y_pred))

"""#SVM2"""

# from sklearn import svm
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report
# from sklearn.utils import class_weight
# import numpy as np

# # Step 1: Preprocess your text data and prepare your labels
# # Assuming you have your text data stored in a list called 'texts' and corresponding labels in 'labels'

# # Step 2: Convert text data into numerical feature vectors
# vectorizer = TfidfVectorizer()  # You can customize the vectorizer based on your specific requirements
# X_train = vectorizer.fit_transform(train).toarray()
# X_test = vectorizer.transform(test1).toarray()
# y_train = train_lab
# y_test = labels

# # Step 3: Split the data into training and testing sets
# # X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

# # Step 4: Calculate class weights to account for imbalance
# class_labels = np.unique(y_train)
# class_weights = class_weight.compute_class_weight('balanced', classes=class_labels, y=y_train)

# # Create a dictionary with class labels and corresponding weights
# class_weights_dict = {class_labels[i]: weight for i, weight in enumerate(class_weights)}

# # Step 5: Train the SVM model with class weights
# svm_model2 = svm.SVC(kernel='linear', class_weight=class_weights_dict)
# svm_model2.fit(X_train, y_train)

# # Step 6: Evaluate the model
# SVM2_y_pred = svm_model2.predict(X_test)
# print(classification_report(y_test, SVM2_y_pred))

"""#Roberta with class weights in loss function"""

# import torch
# from torch.utils.data import Dataset
# from transformers import AutoTokenizer
# from transformers import AutoModelForSequenceClassification
# from transformers import Trainer
# from transformers import TrainingArguments
# from sklearn.preprocessing import LabelEncoder
# import xgboost as xgb
# from sklearn.metrics import accuracy_score

# class SentimentDataset(Dataset):
#     def __init__(self, encodings: dict, labels: torch.Tensor):
#         self.encodings = encodings
#         self.labels = labels

#     def __len__(self) -> int:
#         return len(self.encodings["input_ids"])

#     def __getitem__(self, idx: int) -> dict:
#         e = {k: v[idx] for k, v in self.encodings.items()}
#         e["label"] = self.labels[idx]
#         return e

# AT2 = AutoTokenizer.from_pretrained("roberta-base")

# le = LabelEncoder()

# # Transform the train labels to numerical representations
# train_labels = le.fit_transform(train_data["Label"])
# val_labels = le.fit_transform(val_data["Class labels"])
# test_labels = le.fit_transform(test_data["Label"])

# # Train Inputs
# Roberta2_train_encodings = AT2(
#     train_data["cleaned_data"].tolist(),
#     padding=True,
#     max_length=128,
#     return_tensors="pt",
#     truncation=True
# )
# train_dataset = SentimentDataset(Roberta2_train_encodings, torch.tensor(train_labels))

# # Val Inputs
# val_encodings = AT2(
#     val_data["cleaned_data"].tolist(),
#     padding=True,
#     max_length=128,
#     return_tensors="pt",
#     truncation=True
# )
# val_dataset = SentimentDataset(val_encodings, torch.tensor(val_labels))

# # Test Inputs
# test_encodings = AT2(
#     test_data["cleaned_data"].tolist(),
#     padding=True,
#     max_length=128,
#     return_tensors="pt",
#     truncation=True
# )
# test_dataset = SentimentDataset(test_encodings, torch.tensor(test_labels))

# num_labels = 3  # Set the number of labels for classification
# model_name = "roberta-base"  # Choose the RoBERTa model variant
# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

# training_args = TrainingArguments(
#     output_dir="./results",
#     num_train_epochs=10,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     lr_scheduler_type='cosine',
#     per_device_train_batch_size=32,
#     per_device_eval_batch_size=32,
#     fp16=True,
# )

# Roberta2_trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     data_collator=lambda data: {"input_ids": torch.stack([item["input_ids"] for item in data]),
#                                "attention_mask": torch.stack([item["attention_mask"] for item in data]),
#                                "labels": torch.tensor([item["label"] for item in data])},
#     compute_metrics=lambda pred: {"accuracy": (pred.predictions.argmax(-1) == pred.label_ids).mean()}
# )

# # Train the model
# Roberta2_trainer.train()

# # Get predictions on test dataset
# predictions = Roberta2_trainer.predict(test_dataset)
# test_predictions = predictions.predictions.argmax(-1)

# # Apply softmax to model outputs
# softmax = torch.nn.Softmax(dim=1)
# test_predictions_softmax = softmax(torch.from_numpy(predictions.predictions))

# # Convert torch tensor to numpy array for XGBoost
# train_X = Roberta2_trainer.get_train_dataloader().dataset.encodings["input_ids"].numpy()
# train_y = Roberta2_trainer.get_train_dataloader().dataset.labels.numpy()
# val_X = val_dataset.encodings["input_ids"].numpy()
# val_y = val_dataset.labels.numpy()

# # Train XGBoost model
# xgb_model = xgb.XGBClassifier()
# xgb_model.fit(train_X, train_y)

# # Make predictions using XGBoost model
# xgb_test_predictions = xgb_model.predict(test_encodings["input_ids"].numpy())

# # Calculate accuracy for XGBoost model
# xgb_test_accuracy = accuracy_score(test_labels, xgb_test_predictions)
# xgb_test_accuracy

#  #Refrence- Advanced NLP Lab 03 Solutions provided on Blackboard
# import numpy as np
# from sklearn.metrics import classification_report

# Roberta2_preds = Roberta2_trainer.predict(test_dataset)
# # print(preds)

# Roberta2_preds = le.inverse_transform(np.argmax(Roberta2_preds.predictions, axis=1))
# # print(preds)
# # print(val_data["Class labels"])
# print(classification_report(test_data["Label"].tolist(), Roberta2_preds))

"""#DeBERTa"""

import torch
from torch.utils.data import Dataset
from transformers import DebertaTokenizer
from transformers import DebertaForSequenceClassification
from transformers import Trainer
from transformers import TrainingArguments
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score

class SentimentDataset(Dataset):
    def __init__(self, encodings: dict, labels: torch.Tensor):
        self.encodings = encodings
        self.labels = labels

    def __len__(self) -> int:
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx: int) -> dict:
        e = {k: v[idx] for k, v in self.encodings.items()}
        e["label"] = self.labels[idx]
        return e

AT3 = DebertaTokenizer.from_pretrained("microsoft/deberta-base")

le = LabelEncoder()

# Transform the train labels to numerical representations
train_labels = le.fit_transform(train_data["Label"])
val_labels = le.fit_transform(val_data["Class labels"])
test_labels = le.fit_transform(test_data["Label"])

# Train Inputs
DeRoberta_train_encodings = AT3(
    train_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
train_dataset = SentimentDataset(DeRoberta_train_encodings, torch.tensor(train_labels))

# Val Inputs
DeRoberta_val_encodings = AT3(
    val_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
val_dataset = SentimentDataset(DeRoberta_val_encodings, torch.tensor(val_labels))

# Test Inputs
DeRoberta_test_encodings = AT3(
    test_data["cleaned_data"].tolist(),
    padding=True,
    max_length=128,
    return_tensors="pt",
    truncation=True
)
test_dataset = SentimentDataset(DeRoberta_test_encodings, torch.tensor(test_labels))

num_labels = 3  # Set the number of labels for classification
model_name = "microsoft/deberta-base"  # Choose the DeBERTa model variant
model = DebertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type='cosine',
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    fp16=True,
)

DeRoberta_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=lambda data: {"input_ids": torch.stack([item["input_ids"] for item in data]),
                               "attention_mask": torch.stack([item["attention_mask"] for item in data]),
                               "labels": torch.tensor([item["label"] for item in data])},
    compute_metrics=lambda pred: {"accuracy": (pred.predictions.argmax(-1) == pred.label_ids).mean()}
)

# Train the model
DeRoberta_trainer.train()

# Get predictions on test dataset
predictions = DeRoberta_trainer.predict(test_dataset)
test_predictions = predictions.predictions.argmax(-1)

#Refrence- Advanced NLP Lab 03 Solutions provided on Blackboard
import numpy as np
from sklearn.metrics import classification_report

DeRoberta_preds = DeRoberta_trainer.predict(test_dataset)
# print(preds)

DeRoberta_preds = le.inverse_transform(np.argmax(DeRoberta_preds.predictions, axis=1))
# print(preds)
# print(val_data["Class labels"])
print(classification_report(test_data["Label"].tolist(), DeRoberta_preds))

"""#Ensamble method"""

from sklearn.metrics import classification_report

# Assuming you have a list of predictions from base models
base_model_predictions = [BERT_preds, XLMROBERTa_preds, Roberta_preds, DeRoberta_preds]  # Replace with your own predictions

# Voting: Select the most common prediction for each example
ensemble_predictions = []
for i in range(len(base_model_predictions[0])):
    single_example_predictions = [model_preds[i] for model_preds in base_model_predictions]
    most_common_prediction = max(single_example_predictions, key=single_example_predictions.count)
    ensemble_predictions.append(most_common_prediction)

print(classification_report(test_data["Label"].tolist(), ensemble_predictions))

from collections import Counter

# Assuming you have a list of predictions from base models
base_model_predictions = [XLMROBERTa_preds, Roberta_preds, DeRoberta_preds]  # Replace with your own predictions

# Averaging: Compute the average prediction for each example
ensemble_predictions = []
for i in range(len(base_model_predictions[0])):
    single_example_predictions = [model_preds[i] for model_preds in base_model_predictions]
    label_counts = Counter(single_example_predictions)
    most_common_label = label_counts.most_common(1)[0][0]
    ensemble_predictions.append(most_common_label)

print(classification_report(test_data["Label"].tolist(), ensemble_predictions))

from collections import Counter

# Assuming you have a list of predictions from base models
base_model_predictions = [BERT_preds, Roberta_preds, DeRoberta_preds]  # Replace with your own predictions

# Calculate weights based on F1-scores (higher F1-score gets a higher weight)
f1_scores = [f1_score(test_data["Label"].tolist(), preds, average='weighted') for preds in base_model_predictions]
total_f1 = sum(f1_scores)
weights = [f1 / total_f1 for f1 in f1_scores]

# Weighted Majority Voting: Compute the ensemble prediction for each example
ensemble_predictions = []
for i in range(len(base_model_predictions[0])):
    single_example_predictions = [model_preds[i] for model_preds in base_model_predictions]
    weighted_majority = max(single_example_predictions, key=lambda x: sum(weights[j] for j, pred in enumerate(single_example_predictions) if pred == x))
    ensemble_predictions.append(weighted_majority)

print(classification_report(test_data["Label"].tolist(), ensemble_predictions))

